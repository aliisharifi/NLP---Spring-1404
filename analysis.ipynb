{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "092b20a8",
   "metadata": {},
   "source": [
    "# Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3734ef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "from hazm import stopwords_list, Normalizer, word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9234b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './outputs/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586807a2",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "059c35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_dict(dictionary: dict, text_list: list):\n",
    "    for key, value in dictionary.items():\n",
    "        if key in {\n",
    "            'images',\n",
    "            'book_source',\n",
    "            'wikipedia_source',\n",
    "        }:\n",
    "            continue\n",
    "\n",
    "        if isinstance(value, str):\n",
    "            text_list.append(value)\n",
    "        elif isinstance(value, dict):\n",
    "            _handle_dict(value, text_list)\n",
    "        elif isinstance(value, list):\n",
    "            _handle_list(value, text_list)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "def _handle_list(input_list: list, text_list: list):\n",
    "    for x in input_list:\n",
    "        if isinstance(x, str):\n",
    "            text_list.append(x)\n",
    "        elif isinstance(x, dict):\n",
    "            _handle_dict(x, text_list)\n",
    "        elif isinstance(x, list):\n",
    "            _handle_list(x, text_list)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "def extract_text(dictionary: dict):\n",
    "    text_list = []\n",
    "    _handle_dict(dictionary, text_list)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc96382f",
   "metadata": {},
   "source": [
    "## Calculating Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187bd498",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path(DATA_PATH)\n",
    "stopwords = stopwords_list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82292026",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(chain(*[extract_text(json.load(open(path, 'r'))) for path in data_path.rglob('*.json')]))\n",
    "raw_text = '\\n'.join(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1d90fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "normalized_text = normalizer.normalize(raw_text)\n",
    "tokens = word_tokenize(normalized_text)\n",
    "filtered_tokens = [w for w in tokens if w not in stopwords_list()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fda025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of records in dataset: 113\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'number of records in dataset: {}'\n",
    "    .format(len(list(data_path.rglob('*.json'))))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c947cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words count: 17623\n"
     ]
    }
   ],
   "source": [
    "print('words count: {}'.format(len(filtered_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "875c8bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words: 2626\n"
     ]
    }
   ],
   "source": [
    "print('unique words: {}'.format(len(set(filtered_tokens))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "475e5de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average length of sentences: 2.9677971375233354\n"
     ]
    }
   ],
   "source": [
    "print('average length of sentences: {}'.format(sum(map(lambda x: len(x.split()), sentences)) / len(sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af419e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-spring1404-juIwDZ-T-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
